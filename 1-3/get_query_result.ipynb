{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本加载为json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/tweets.txt'\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "items = [json.loads(x) for x in lines]\n",
    "tweets = [x['text'] for x in items]\n",
    "idx2id = [x['tweetId'] for x in items]\n",
    "N = len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义预处理类\n",
    "* 大写转小写\n",
    "* 分词\n",
    "* 去除标点符号和停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.punctuations = [',',':','_','!','\\\"','*','>','<','@','~','-','(',')','%','=','\\\\','^','&','|','#','$','[',']','+',':','#','|'] \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    def __call__(self,text,query=False):\n",
    "        text = text.lower()\n",
    "        text = nltk.word_tokenize(text)\n",
    "        text = [x for x in text if x not in self.punctuations and x not in self.stop_words]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess()\n",
    "tokens = [preprocess(x) for x in tweets] # 每个doc的分词\n",
    "length = [nltk.FreqDist(x) for x in tokens] # 每个doc的词频统计信息\n",
    "length = [list(x.values()) for x in length] # 每个doc的tf向量\n",
    "length = [math.sqrt(sum([tf*tf for tf in x])) for x in length] # 每个doc的tf向量的L2范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "# posting 包括文档编号和tf\n",
    "for i, token in enumerate(tokens):\n",
    "    term_freq = nltk.FreqDist(token)\n",
    "    for term, freq in term_freq.items():\n",
    "        if term in dictionary:\n",
    "            dictionary[term].add((i,freq))\n",
    "        else:\n",
    "            dictionary[term] = {(i,freq)}\n",
    "# 按tf由大到小排序\n",
    "for k,v in dictionary.items():\n",
    "    dictionary[k] = sorted(list(dictionary[k]),key=lambda x:(x[1],x[0]),reverse=True)\n",
    "\n",
    "def get_postings(term):\n",
    "    if term in dictionary:\n",
    "        return dictionary[term]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义查询运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_parse(query):\n",
    "    query = query.lower()\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "    postings = set()\n",
    "    for token in tokens:\n",
    "        postings.update(get_postings(token))\n",
    "    postings = list(postings)\n",
    "    postings.sort(key=lambda x:(x[1],x[0]),reverse=True)\n",
    "    return postings\n",
    "\n",
    "# 带红色强调字体输出\n",
    "def toRed( s ):\n",
    "    return \"%c[31;2m%s%c[0m\"%('\\033', s, '\\033')\n",
    "def print_with_emphasize(line, Q):\n",
    "    Q = Q.lower()\n",
    "    line = nltk.word_tokenize(line)\n",
    "    s_line = nltk.word_tokenize(Q)\n",
    "    to_be_print = ''\n",
    "    for l in line:\n",
    "        if l.lower() in s_line:\n",
    "            to_be_print += toRed(l) + ' '\n",
    "        else:\n",
    "            to_be_print += l + ' '\n",
    "    print(to_be_print+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义top K运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = {\n",
    "    'n':lambda tf:tf,\n",
    "    'l':lambda tf:[1+math.log(x) if x>=1 else 0 for x in tf],\n",
    "    'a':lambda tf:[0.5+0.5*x/max(tf) if max(tf)>0 else 0 for x in tf],\n",
    "    'b':lambda tf:[1 if x>0 else 0 for x in tf],\n",
    "    # TODO 未实现\n",
    "    'L':lambda tf:[(1+math.log(x))/(1+math.log(sum(tf)/len(tf))) for x in tf]\n",
    "}\n",
    "\n",
    "DF = {\n",
    "    'n':lambda df:[1]*len(df),\n",
    "    't':lambda df:[math.log(N/x) if x>=1 else 0 for x in df],\n",
    "    'p':lambda df:[max(0,math.log((N-x)/x)) if x>=1 else 0 for x in df]\n",
    "}\n",
    "\n",
    "NORM = {\n",
    "    'n':lambda tfdf:[1]*len(tfdf),\n",
    "    'c':lambda tfdf:[1/math.sqrt(sum([w*w for w in w_list])) if sum(w_list)>0 else 0]*len(tfdf),\n",
    "    \n",
    "    # TODO 未实现\n",
    "    'u':lambda tfdf:1,\n",
    "    'b':lambda tfdf:1\n",
    "}\n",
    "\n",
    "def compute_wtq(terms, query_term_freq, notation='ltn'):\n",
    "    tf = [query_term_freq[term] for term in terms]\n",
    "    tf = TF[notation[0]](tf)\n",
    "    df = [len(get_postings(term)) for term in terms]\n",
    "    df = DF[notation[1]](df)\n",
    "    tfdf = [tf[i]*df[i] for i in range(len(tf))]\n",
    "    norm = NORM[notation[2]](tfdf)\n",
    "    w = [tfdf[i] * norm[i] for i in range(len(tfdf))]\n",
    "    return w\n",
    "\n",
    "def compute_wtd(tf,df,notation='lnc'):\n",
    "    tf = TF[notation[0]]([tf])[0]\n",
    "    df = DF[notation[1]]([df])[0]\n",
    "    return tf*df\n",
    "\n",
    "def top_k(query,k,notation='lnc.ltn'):\n",
    "    notation = notation.split('.')\n",
    "    query_tokens = preprocess(query)\n",
    "    term_freq = nltk.FreqDist(query_tokens)\n",
    "    query_terms = list(term_freq.keys())\n",
    "    score = [0]*N\n",
    "    wtq = compute_wtq(query_terms, term_freq, notation[1])\n",
    "    for i in range(len(query_terms)):\n",
    "        postings = get_postings(query_terms[i])\n",
    "        for posting in postings:\n",
    "            # wtd未normalize，在循环外normalize\n",
    "            wtd = compute_wtd(posting[1],len(postings),notation[0])\n",
    "            score[posting[0]] += wtq[i]*wtd\n",
    "    \n",
    "    # document normalization 只实现n和c\n",
    "    if notation[0][2] == 'c':\n",
    "        score = [score[i]/length[i] for i in range(N)]\n",
    "    \n",
    "    # 修正k，只返回相关结果\n",
    "    k_correct = len([x for x in score if x > 0])\n",
    "    if k > k_correct:\n",
    "        k = k_correct\n",
    "        \n",
    "    score = np.array(score)\n",
    "    order = list(np.argsort(score))\n",
    "    order.reverse()\n",
    "    results = [tweets[i] for i in order[:k]]\n",
    "    ids = [idx2id[i] for i in order[:k]]\n",
    "    return results, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_result(query, k, notation='lnc.ltn'):\n",
    "    results, ids = top_k(query,k,notation)\n",
    "    print(\"查询%s, 返回前%s条结果(%s)：\\n\"%(toRed(query), toRed(str(len(results))), toRed(notation)))\n",
    "    for idx, result in enumerate(results):\n",
    "        print(\"tweet id: {}\".format(ids[idx]))\n",
    "        print_with_emphasize(result,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载问题关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = dict()\n",
    "with open('data/topics.desc.MB171-225.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for idx,line in enumerate(lines):\n",
    "        if line[:5] == '<num>':\n",
    "            num = line.split()[2][-3:]\n",
    "            query = lines[idx+1][8:-10]\n",
    "            queries[num] = query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "notation = 'lnc.ltn'\n",
    "with open('result/my_result_lnc.ltn.txt', 'w') as f:\n",
    "    for query_id in queries:\n",
    "        results, ids = top_k(queries[query_id], k, notation)\n",
    "        for doc_id in ids:\n",
    "            f.write('{} {}\\n'.format(query_id, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "notation = 'bpc.ltn'\n",
    "with open('result/my_result_bpc.ltn.txt', 'w') as f:\n",
    "    for query_id in queries:\n",
    "        results, ids = top_k(queries[query_id], k, notation)\n",
    "        for doc_id in ids:\n",
    "            f.write('{} {}\\n'.format(query_id, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "notation = 'lnn.lnn'\n",
    "with open('result/my_result_lnn.lnn.txt', 'w') as f:\n",
    "    for query_id in queries:\n",
    "        results, ids = top_k(queries[query_id], k, notation)\n",
    "        for doc_id in ids:\n",
    "            f.write('{} {}\\n'.format(query_id, doc_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
