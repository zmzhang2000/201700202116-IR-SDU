{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赛题分析\n",
    "论文的增量消歧(Continuous Name Disambiguation)\n",
    "任务描述：线上系统每天会新增大量的论文，如何准确快速的将论文分配到系统中已有作者档案，这是线上学术系统最亟待解决的问题。所以问题抽象定义为：给定一批新增论文以及系统已有的作者论文集，最终目的是把新增论文分配到正确的作者档案中。\n",
    "参考方法：增量消歧任务与冷启动消歧的任务不同，它是基于有一定作者档案的基础，对新增论文进行分配。所以，容易直接想到的方法就是将已有的作者档案与新增论文进行比较，提取合作者，单位机构或者会议期刊之间相似度的传统特征，随后利用svm之类的传统分类器进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pyjarowinkler import distance\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# 训练集中的作者论文信息\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/train/train_author.json\", \"r\") as f2:\n",
    "    author_data = json.load(f2)\n",
    "\n",
    "# 训练集的论文元信息\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/train/train_pub.json\", \"r\") as f2:\n",
    "    pubs_dict = json.load(f2)\n",
    "\n",
    "print(len(author_data))\n",
    "\n",
    "\n",
    "name_train = set()\n",
    "\n",
    "# 筛选训练集，只取同名作者数大于等于5个的名字作为训练集。\n",
    "for name in author_data:\n",
    "    persons = author_data[name]\n",
    "    if(len(persons) > 5):\n",
    "        name_train.add((name))\n",
    "\n",
    "print(len(name_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198607\n"
     ]
    }
   ],
   "source": [
    "# 采样500个训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "\n",
    "# 记录paper所属作者和名字\n",
    "paper2aid2name = {}\n",
    "\n",
    "for author_name in name_train:\n",
    "    persons = author_data[author_name]\n",
    "    for person in persons:\n",
    "        paper_list = persons[person]\n",
    "        for paper_id in paper_list:\n",
    "            paper2aid2name[paper_id] = (author_name, person)\n",
    "\n",
    "print(len(paper2aid2name))\n",
    "# print(paper2aid2name)\n",
    "\n",
    "total_paper_list = list(paper2aid2name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# 采样10000篇paper作为训练集\n",
    "train_paper_list = random.sample(total_paper_list, 10000)\n",
    "\n",
    "# 把采样的500篇paper转变成对应的训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "train_instances = []\n",
    "for paper_id in train_paper_list:\n",
    "    \n",
    "    # 保存对应的正负例\n",
    "    pos_ins = set()\n",
    "    neg_ins = set()\n",
    "    \n",
    "    paper_author_name = paper2aid2name[paper_id][0]\n",
    "    paper_author_id = paper2aid2name[paper_id][1]\n",
    "    \n",
    "    pos_ins.add((paper_id, paper_author_id))\n",
    "    \n",
    "    # 获取同名的所有作者(除了本身)作为负例的candidate\n",
    "    persons = list(author_data[paper_author_name].keys())\n",
    "    persons.remove(paper_author_id)\n",
    "    assert len(persons) == (len(list(author_data[paper_author_name].keys())) - 1)\n",
    "    \n",
    "    # 每个正例采样5个负例\n",
    "    neg_author_list = random.sample(persons, 1)\n",
    "    for i in neg_author_list:\n",
    "        neg_ins.add((paper_id, i))\n",
    "        \n",
    "    train_instances.append((pos_ins, neg_ins))\n",
    "    \n",
    "print(len(train_instances))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('x34a5vCM', 'WFMrcMVu')}, {('x34a5vCM', 'Np6W5EhI')})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_instances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成特征\n",
    "在这里，我们只提取paper与author之间的coauthor相关的特征:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者所发表过论文的keyword信息\n",
    "aid2keywords = dict()\n",
    "for name in author_data:\n",
    "    for author_id in author_data[name]:\n",
    "        aid2keywords[author_id] = set()\n",
    "        paper_list = author_data[name][author_id]\n",
    "        for paper in paper_list:\n",
    "            if 'keywords' not in pubs_dict[paper]:\n",
    "                continue\n",
    "            for keyword in pubs_dict[paper]['keywords']:\n",
    "                aid2keywords[author_id].add(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里定义了俩个生成特征所需的函数\n",
    "from pyjarowinkler import distance\n",
    "\n",
    "\n",
    "# 对author_name 进行清洗\n",
    "def clean_name(name):\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    x = [k.strip() for k in name.lower().strip().replace(\".\", \"\").replace(\"-\", \" \").replace(\"_\", ' ').split()]\n",
    "    # x = [k.strip() for k in name.lower().strip().replace(\"-\", \"\").replace(\"_\", ' ').split()]\n",
    "    full_name = ' '.join(x)\n",
    "    name_part = full_name.split()\n",
    "    if(len(name_part) >= 1):\n",
    "        return full_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 找出paper中author_name所对应的位置\n",
    "def delete_main_name(author_list, name):\n",
    "    score_list = []\n",
    "    name = clean_name(name)\n",
    "    author_list_lower = []\n",
    "    for author in author_list:\n",
    "        author_list_lower.append(author.lower())\n",
    "    name_split = name.split()\n",
    "    for author in author_list_lower:\n",
    "        # lower_name = author.lower()\n",
    "        score = distance.get_jaro_distance(name, author, winkler=True, scaling=0.1)\n",
    "        author_split = author.split()\n",
    "        inter = set(name_split) & set(author_split)\n",
    "        alls = set(name_split) | set(author_split)\n",
    "        score += round(len(inter)/len(alls), 6)\n",
    "        score_list.append(score)\n",
    "\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    return_list = [author_list_lower[i] for i in rank[1:]]\n",
    "\n",
    "    return return_list, rank[0]\n",
    "\n",
    "# 训练集特征生成函数\n",
    "def process_feature(pos_ins, paper_coauthors, paper_keywords):\n",
    "    # pos_ins: (paper_id, author_id)\n",
    "    # paper_coauthors: [author_name1, author_name2...]\n",
    "    feature_list = []\n",
    "\n",
    "    paper = pos_ins[0] \n",
    "    author = pos_ins[1]\n",
    "    paper_author_id = author\n",
    "    paper_name = paper2aid2name[paper][0]\n",
    "    \n",
    "    # 从作者的论文列表中把该篇论文去掉，防止训练出现bias\n",
    "    doc_list = []\n",
    "    for doc in author_data[paper_name][author]:\n",
    "        if(doc != paper):\n",
    "            doc_list.append(doc)\n",
    "    for doc in doc_list:\n",
    "        if doc == paper:\n",
    "            print(\"error!\")\n",
    "            exit()\n",
    "    \n",
    "    # 保存作者的所有paper的coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        \n",
    "        doc_dict = pubs_dict[doc]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "    \n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            # 获取paper中main author_name所对应的位置\n",
    "            _, author_index = delete_main_name(author_list, paper_name)\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    # author 的所有不同coauthor name\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 6)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "        \n",
    "        co_coauthors_ratio_for_paper = round(coauthor_len / len(paper_coauthors), 6)\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "        \n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "            \n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "\n",
    "        co_keywords_count = len(paper_keywords & aid2keywords[paper_author_id])/len(aid2keywords)\n",
    "        \n",
    "        # 计算了5维paper与author所有的paper的coauthor相关的特征：\n",
    "        #    1. 不重复的coauthor个数\n",
    "        #    2. 不重复的coauthor个数 / paper的所有coauthor的个数\n",
    "        #    3. 不重复的coauthor个数 / author的所有paper不重复coauthor的个数\n",
    "        #    4. coauthor个数（含重复）\n",
    "        #    5. coauthor个数（含重复）/ author的所有paper的coauthor的个数（含重复）\n",
    "        #    6. cokeyword个数\n",
    "        feature_list.extend([coauthor_len, co_coauthors_ratio_for_paper, co_coauthors_ratio_for_author, coauthor_count, co_coauthors_ratio_for_author_count, co_keywords_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6121081e11444bd299a7fe60db332f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(9998, 6)\n",
      "(9998, 6)\n"
     ]
    }
   ],
   "source": [
    "#生成所有正例以及负例的特征\n",
    "from collections import defaultdict\n",
    "\n",
    "pos_features = []\n",
    "neg_features = []\n",
    "\n",
    "print(len(train_instances))\n",
    "\n",
    "for ins in tqdm(train_instances):\n",
    "    \n",
    "    pos_set = ins[0]\n",
    "    neg_set = ins[1]\n",
    "    paper_id = list(pos_set)[0][0]\n",
    "    paper_name = paper2aid2name[paper_id][0]\n",
    "    \n",
    "    \n",
    "    author_list = []\n",
    "    # 获取paper的coauthors\n",
    "    paper_coauthors = []\n",
    "    paper_keywords = set()\n",
    "    if 'keywords' in pubs_dict[paper_id]:\n",
    "        paper_keywords.update(pubs_dict[paper_id]['keywords'])\n",
    "\n",
    "    paper_authors = pubs_dict[paper_id]['authors']\n",
    "    paper_authors_len = len(paper_authors)\n",
    "    # 只取前50个author以保证效率\n",
    "    paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "    for author in paper_authors:                \n",
    "        clean_author = clean_name(author['name'])\n",
    "        if(clean_author != None):\n",
    "            author_list.append(clean_author)\n",
    "    if(len(author_list) > 0):\n",
    "        # 获取paper中main author_name所对应的位置\n",
    "        _, author_index = delete_main_name(author_list, paper_name)\n",
    "        \n",
    "        # 获取除了main author_name外的coauthor\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "        \n",
    "    \n",
    "        for pos_ins in pos_set:\n",
    "            pos_features.append(process_feature(pos_ins, paper_coauthors, paper_keywords))\n",
    "    \n",
    "        for neg_ins in neg_set:\n",
    "            neg_features.append(process_feature(neg_ins, paper_coauthors, paper_keywords))\n",
    "            \n",
    "print(np.array(pos_features).shape)\n",
    "print(np.array(neg_features).shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9a3478ddc94cf082ceccc1258b6dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9998), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(19996, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa18ad3073341549f5292892c4ee619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19996), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建正负例\n",
    "train_ins = []\n",
    "for ins in tqdm(pos_features):\n",
    "    train_ins.append((ins, 1))\n",
    "\n",
    "for ins in neg_features:\n",
    "    train_ins.append((ins, 0))\n",
    "\n",
    "print(np.array(train_ins).shape)\n",
    "\n",
    "random.shuffle(train_ins)\n",
    "\n",
    "x_train= []\n",
    "y_train = [] \n",
    "for ins in tqdm(train_ins):\n",
    "    x_train.append(ins[0])\n",
    "    y_train.append(ins[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "# 训练集中的作者论文信息\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/whole_author_profile.json\", \"r\") as f2:\n",
    "    test_author_data = json.load(f2)\n",
    "\n",
    "# 训练集的论文元信息\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/whole_author_profile_pub.json\", \"r\") as f2:\n",
    "    test_pubs_dict = json.load(f2)\n",
    "\n",
    "# 待分配论文集\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/cna_valid_unass_competition.json\", \"r\") as f2:\n",
    "    unass_papers = json.load(f2)\n",
    "\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/cna_valid_pub.json\", \"r\") as f2:\n",
    "    unass_papers_dict = json.load(f2)\n",
    "\n",
    "# with open(\"cna_data/new_test_author_data.json\", 'r') as files:\n",
    "#     new_test_author_data = json.load(files)\n",
    "# 简单处理whole_author_profile，将同名的作者合并：\n",
    "# 为了效率，预处理new_test_author_data中的paper，将其全部处理成paper_id + '-' + author_index的形式。\n",
    "new_test_author_data = {}\n",
    "for author_id, author_info in test_author_data.items():\n",
    "    author_name = author_info['name']\n",
    "    author_papers = author_info['papers']\n",
    "    newly_papers = []\n",
    "\n",
    "    for paper_id in author_papers:\n",
    "\n",
    "        paper_authors = test_pubs_dict[paper_id]['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        \n",
    "        # 只利用author数小于50的paper，以保证效率\n",
    "        if(paper_authors_len > 50):\n",
    "            continue\n",
    "#         paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "        author_list = []\n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            # 获取paper中main author_name所对应的位置\n",
    "            _, author_index = delete_main_name(author_list, paper_name)\n",
    "\n",
    "            new_paper_id = str(paper_id) + '-' + str(author_index)\n",
    "            newly_papers.append(new_paper_id)\n",
    "        \n",
    "        \n",
    "    if(new_test_author_data.get(author_name) != None):\n",
    "        new_test_author_data[author_name][author_id] = newly_papers\n",
    "    else:\n",
    "        tmp = {}\n",
    "        tmp[author_id] = newly_papers\n",
    "        new_test_author_data[author_name] = tmp\n",
    "print(len(new_test_author_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test集的特征生成函数，与train类似\n",
    "def process_test_feature(pair, new_test_author_data, test_pubs_dict, paper_coauthors, paper_keywords):\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    paper = pair[0] \n",
    "    author = pair[1]\n",
    "    paper_name = pair[2]\n",
    "    \n",
    "    doc_list = new_test_author_data[paper_name][author]\n",
    "\n",
    "    \n",
    "    # 保存作者的所有coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        doc_id = doc.split('-')[0]\n",
    "        author_index = doc.split('-')[1]\n",
    "        doc_dict = test_pubs_dict[doc_id]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "    \n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 6)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "        \n",
    "        co_coauthors_ratio_for_paper = round(coauthor_len / len(paper_coauthors), 6)\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "        \n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "            \n",
    "        \n",
    "        \n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "        \n",
    "        co_keywords_count = len(set(paper_keywords) & aid2keywords[paper_author_id])\n",
    "\n",
    "        # 计算了5维paper与author所有的paper的coauthor相关的特征：\n",
    "        #    1. 不重复的coauthor个数\n",
    "        #    2. 不重复的coauthor个数 / paper的所有coauthor的个数\n",
    "        #    3. 不重复的coauthor个数 / author的所有paper不重复coauthor的个数\n",
    "        #    4. coauthor个数（含重复）\n",
    "        #    4. coauthor个数（含重复）/ author的所有paper的coauthor的个数（含重复）\n",
    "        feature_list.extend([coauthor_len, co_coauthors_ratio_for_paper, co_coauthors_ratio_for_author, coauthor_count, co_coauthors_ratio_for_author_count, co_keywords_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9170\n",
      "8792\n",
      "8761\n"
     ]
    }
   ],
   "source": [
    "print(len(unass_papers))\n",
    "\n",
    "count = 0\n",
    "\n",
    "# 存储paper的所有candidate author id\n",
    "paper2candidates = defaultdict(list)\n",
    "# 存储对应的paper与candidate author的生成特征\n",
    "paper2features = defaultdict(list)\n",
    "\n",
    "for u_p in unass_papers:\n",
    "    paper_id = u_p.split('-')[0]\n",
    "    author_index = int(u_p.split('-')[1])\n",
    "    author_list = []\n",
    "    \n",
    "    # 获取paper的coauthors\n",
    "    paper_coauthors = []\n",
    "    paper_name = ''\n",
    "    paper_authors = unass_papers_dict[paper_id]['authors']\n",
    "    paper_keywords = unass_papers_dict[paper_id]['keywords']\n",
    "#     paper_authors_len = len(paper_authors)\n",
    "#     paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "    for author in paper_authors:                \n",
    "        clean_author = clean_name(author['name'])\n",
    "        if(clean_author != None):\n",
    "            author_list.append(clean_author)\n",
    "    if(len(author_list) > 0):\n",
    "        \n",
    "        # 获取除了main author_name外的coauthor\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "    \n",
    "    # 简单使用精确匹配找出candidate_author_list\n",
    "    paper_name = '_'.join(clean_name(paper_authors[author_index]['name']).split())\n",
    "    if(new_test_author_data.get(paper_name) != None):\n",
    "        candidate_author_list = new_test_author_data[paper_name]\n",
    "        for candidate_author in candidate_author_list:\n",
    "            pair = (paper_id, candidate_author, paper_name)\n",
    "            paper2candidates[paper_id].append(candidate_author)\n",
    "            paper2features[paper_id].append(process_test_feature(pair, new_test_author_data, test_pubs_dict, paper_coauthors, paper_keywords))\n",
    "        count += 1\n",
    "print(count)\n",
    "assert len(paper2candidates) == len(paper2features)\n",
    "print(len(paper2candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainData(data.Dataset):\n",
    "    def __init__(self, instances):\n",
    "        self.instances = instances\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature, label = self.instances[index]\n",
    "        return torch.FloatTensor(feature).cuda(), torch.FloatTensor([label]).cuda()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "class testData(data.Dataset):\n",
    "    def __init__(self, instances):\n",
    "        self.instances = instances\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.instances[index]\n",
    "        return torch.FloatTensor(feature).cuda()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15997 3999\n"
     ]
    }
   ],
   "source": [
    "split = int(len(train_ins)/5)\n",
    "train_data, test_data = train_ins[split:], train_ins[:split]\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "train_data = trainData(train_data)\n",
    "train_loader = data.DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "test_data = trainData(test_data)\n",
    "test_loader = data.DataLoader(test_data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(6,10,10)\n",
    "net = net.cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3,3.0])\n",
    "3 in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a090402132485480c23365065971db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999],\n",
      "        [0.6509],\n",
      "        [0.5895],\n",
      "        [1.0000]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "\n",
      "epoch 0: nan\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(1):\n",
    "    loss_epoch = []\n",
    "    for feature, label in tqdm(train_loader):\n",
    "        pred = net(feature)\n",
    "        if 0 in pred or 1 in pred:\n",
    "            print(pred)\n",
    "        loss = (-label*torch.log(pred)-(1-label)*torch.log(1-pred)).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch.append(loss.item())\n",
    "    print(f\"epoch {epoch}: {sum(loss_epoch)/len(loss_epoch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944a14fbf6d54ad1a36d02aa39962afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 0.5011252813203301\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "result = []\n",
    "for feature, label in tqdm(test_loader):\n",
    "    pred = net(feature)\n",
    "    for idx in range(pred.shape[0]):\n",
    "        if (pred[idx].item() >= 0.5 and label[idx].item()==1) or (pred[idx].item()<0.5 and label[idx].item()==0):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "acc = sum(result)/len(result)\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用svm进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\biendata\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "D:\\ProgramData\\Anaconda3\\envs\\biendata\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用训练好的svm模型去预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paper2features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-b517df52b727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresult_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mins_feature_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaper2features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mscore_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mins_feature_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# 利用svm对一篇paper的所有candidate author去打分，利用分数进行排序，取top-1 author作为预测的author\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'paper2features' is not defined"
     ]
    }
   ],
   "source": [
    "result_dict = defaultdict(list)\n",
    "for paper_id, ins_feature_list in paper2features.items(): \n",
    "    score_list = []\n",
    "    for ins in ins_feature_list:\n",
    "        # 利用svm对一篇paper的所有candidate author去打分，利用分数进行排序，取top-1 author作为预测的author\n",
    "        prob_pred = clf.predict_proba([ins])[:, 1]\n",
    "        score_list.append(prob_pred[0])\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    #取top-1 author作为预测的author\n",
    "    predict_author = paper2candidates[paper_id][rank[0]]\n",
    "    result_dict[predict_author].append(paper_id)\n",
    "\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/result.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提交评测后，结果为F1 = 0.63100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(x_train):\n",
    "    print(x, y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    " \n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "for paper_id, ins_feature_list in paper2features.items(): \n",
    "    score_list = []\n",
    "    for ins in ins_feature_list:\n",
    "        # 利用svm对一篇paper的所有candidate author去打分，利用分数进行排序，取top-1 author作为预测的author\n",
    "        prob_pred = clf_decision_tree.predict_proba([ins])[:, 1]\n",
    "        score_list.append(prob_pred[0])\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    #取top-1 author作为预测的author\n",
    "    predict_author = paper2candidates[paper_id][rank[0]]\n",
    "    result_dict[predict_author].append(paper_id)\n",
    "\n",
    "with open(\"D:/Zhang/Datasets/论文增量消歧/cna_data/result.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biendata]",
   "language": "python",
   "name": "conda-env-biendata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
