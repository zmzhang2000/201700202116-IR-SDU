{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本加载为json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'tweets.txt'\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "items = [json.loads(x) for x in lines]\n",
    "tweets = [x['text'] for x in items]\n",
    "N = len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义预处理类\n",
    "* 大写转小写\n",
    "* 分词\n",
    "* 去除标点符号和停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.punctuations = [',',':','_','!','\\\"','*','>','<','@','~','-','(',')','%','=','\\\\','^','&','|','#','$','[',']','+',':','#','|'] \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    def __call__(self,text,query=False):\n",
    "        text = text.lower()\n",
    "        text = nltk.word_tokenize(text)\n",
    "        text = [x for x in text if x not in self.punctuations and x not in self.stop_words]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess()\n",
    "tokens = [preprocess(x) for x in tweets] # 每个doc的分词\n",
    "length = [nltk.FreqDist(x) for x in tokens] # 每个doc的词频统计信息\n",
    "length = [list(x.values()) for x in length] # 每个doc的tf向量\n",
    "length = [math.sqrt(sum([tf*tf for tf in x])) for x in length] # 每个doc的tf向量的L2范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "# posting 包括文档编号和tf\n",
    "for i, token in enumerate(tokens):\n",
    "    term_freq = nltk.FreqDist(token)\n",
    "    for term, freq in term_freq.items():\n",
    "        if term in dictionary:\n",
    "            dictionary[term].add((i,freq))\n",
    "        else:\n",
    "            dictionary[term] = {(i,freq)}\n",
    "# 按tf由大到小排序\n",
    "for k,v in dictionary.items():\n",
    "    dictionary[k] = sorted(list(dictionary[k]),key=lambda x:(x[1],x[0]),reverse=True)\n",
    "\n",
    "def get_postings(term):\n",
    "    if term in dictionary:\n",
    "        return dictionary[term]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义查询运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_parse(query):\n",
    "    query = query.lower()\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "    postings = set()\n",
    "    for token in tokens:\n",
    "        postings.update(get_postings(token))\n",
    "    postings = list(postings)\n",
    "    postings.sort(key=lambda x:(x[1],x[0]),reverse=True)\n",
    "    return postings\n",
    "\n",
    "# 带红色强调字体输出\n",
    "def toRed( s ):\n",
    "    return \"%c[31;2m%s%c[0m\"%('\\033', s, '\\033')\n",
    "def print_with_emphasize(line, Q):\n",
    "    Q = Q.lower()\n",
    "    line = nltk.word_tokenize(line)\n",
    "    s_line = nltk.word_tokenize(Q)\n",
    "    to_be_print = ''\n",
    "    for l in line:\n",
    "        if l.lower() in s_line:\n",
    "            to_be_print += toRed(l) + ' '\n",
    "        else:\n",
    "            to_be_print += l + ' '\n",
    "    print(to_be_print+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义top K运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = {\n",
    "    'n':lambda tf:tf,\n",
    "    'l':lambda tf:[1+math.log(x) if x>=1 else 0 for x in tf],\n",
    "    'a':lambda tf:[0.5+0.5*x/max(tf) if max(tf)>0 else 0 for x in tf],\n",
    "    'b':lambda tf:[1 if x>0 else 0 for x in tf],\n",
    "    # TODO 未实现\n",
    "    'L':lambda tf:[(1+math.log(x))/(1+math.log(sum(tf)/len(tf))) for x in tf]\n",
    "}\n",
    "\n",
    "DF = {\n",
    "    'n':lambda df:[1]*len(df),\n",
    "    't':lambda df:[math.log(N/x) if x>=1 else 0 for x in df],\n",
    "    'p':lambda df:[max(0,math.log((N-x)/x)) if x>=1 else 0 for x in df]\n",
    "}\n",
    "\n",
    "NORM = {\n",
    "    'n':lambda tfdf:[1]*len(tfdf),\n",
    "    'c':lambda tfdf:[1/math.sqrt(sum([w*w for w in w_list])) if sum(w_list)>0 else 0]*len(tfdf),\n",
    "    \n",
    "    # TODO 未实现\n",
    "    'u':lambda tfdf:1,\n",
    "    'b':lambda tfdf:1\n",
    "}\n",
    "\n",
    "def compute_wtq(terms, query_term_freq, notation='ltn'):\n",
    "    tf = [query_term_freq[term] for term in terms]\n",
    "    tf = TF[notation[0]](tf)\n",
    "    df = [len(get_postings(term)) for term in terms]\n",
    "    df = DF[notation[1]](df)\n",
    "    tfdf = [tf[i]*df[i] for i in range(len(tf))]\n",
    "    norm = NORM[notation[2]](tfdf)\n",
    "    w = [tfdf[i] * norm[i] for i in range(len(tfdf))]\n",
    "    return w\n",
    "\n",
    "def compute_wtd(tf,df,notation='lnc'):\n",
    "    tf = TF[notation[0]]([tf])[0]\n",
    "    df = DF[notation[1]]([df])[0]\n",
    "    return tf*df\n",
    "\n",
    "def top_k(query,k,notation='lnc.ltn'):\n",
    "    notation = notation.split('.')\n",
    "    query_tokens = preprocess(query)\n",
    "    term_freq = nltk.FreqDist(query_tokens)\n",
    "    query_terms = list(term_freq.keys())\n",
    "    score = [0]*N\n",
    "    wtq = compute_wtq(query_terms, term_freq, notation[1])\n",
    "    for i in range(len(query_terms)):\n",
    "        postings = get_postings(query_terms[i])\n",
    "        for posting in postings:\n",
    "            # wtd未normalize，在循环外normalize\n",
    "            wtd = compute_wtd(posting[1],len(postings),notation[0])\n",
    "            score[posting[0]] += wtq[i]*wtd\n",
    "    \n",
    "    # document normalization 只实现n和c\n",
    "    if notation[0][2] == 'c':\n",
    "        score = [score[i]/length[i] for i in range(N)]\n",
    "    \n",
    "    # 修正k，只返回相关结果\n",
    "    k_correct = len([x for x in score if x > 0])\n",
    "    if k > k_correct:\n",
    "        k = k_correct\n",
    "        \n",
    "    score = np.array(score)\n",
    "    order = list(np.argsort(score))\n",
    "    order.reverse()\n",
    "    results = [tweets[i] for i in order[:k]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_result(query, k):\n",
    "    results = top_k(query,k)\n",
    "    print(\"查询%s, 返回前%s条结果：\\n\"%(toRed(query), toRed(str(len(results)))))\n",
    "    for result in results:\n",
    "        print_with_emphasize(result,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 显示查询交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = tk.Tk()\n",
    "win.title('查询（请输入关键词和k）')\n",
    "win.geometry('700x100')\n",
    "\n",
    "query = tk.StringVar()\n",
    "entry1 = tk.Entry(win, relief='sunken',font='Calibri 23',width=33,textvariable=query)\n",
    "entry1.place(relx=0.40,rely=0.5,anchor='center')\n",
    "k = tk.IntVar(value=10)\n",
    "entry2 = tk.Entry(win, relief='sunken',font='Calibri 23',width=3,textvariable=k)\n",
    "entry2.place(relx=0.83,rely=0.5,anchor='center')\n",
    "\n",
    "def button_command():\n",
    "    clear_output()\n",
    "    query_sentence = query.get()\n",
    "    k_value = k.get()\n",
    "    display_query_result(query_sentence, k_value)\n",
    "\n",
    "button = tk.Button(win, text='查询', height=2,width=8,command=button_command)\n",
    "button.place(relx=0.93,rely=0.5,anchor='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询\u001b[31;2mmuscle pain from statins\u001b[0m, 返回前\u001b[31;2m15\u001b[0m条结果：\n",
      "\n",
      "Is your \u001b[31;2mmuscle\u001b[0m \u001b[31;2mpain\u001b[0m related to cholesterol medication ? If so then a common over the counter supplement may help ... http : //t.co/AwCR0ryu \n",
      "\n",
      "Bloomberg : `` Merck ’ s Tredaptive Raises Risk of \u001b[31;2mMuscle\u001b[0m \u001b[31;2mPain\u001b[0m in Study '' - report of HPS2-THRIVE study in # EHJ today : http : //t.co/R5MdwpAMF6 \n",
      "\n",
      "PHOTOS : \u001b[31;2mMuscle\u001b[0m cars raise $ 10000 for Queensland floods : The American \u001b[31;2mMuscle\u001b[0m Car Club of Australia recen ... http : //bit.ly/dOdSDo # muscles \n",
      "\n",
      "Diabetes Patients with Chest \u001b[31;2mPain\u001b[0m http : //t.co/oUEyCzEQP8 \n",
      "\n",
      "@ DuncanBannatyne bbc cut backs are a \u001b[31;2mpain\u001b[0m ... \n",
      "\n",
      "\u001b[31;2mStatins\u001b[0m often prescribed without good evidence : Many doctors prescribe \u001b[31;2mstatins\u001b[0m to people who have little chanc ... http : //t.co/8Wx5Y9vYOU \n",
      "\n",
      "TV Ads for \u001b[31;2mStatins\u001b[0m Drive Overdiagnosis and Overtreatment http : //t.co/TbpgIZJv4E \n",
      "\n",
      "Mediterranean diet 'as good as \u001b[31;2mstatins\u001b[0m ' http : //t.co/pzrm886hyg \n",
      "\n",
      "\u001b[31;2mStatins\u001b[0m often prescribed without good evidence http : //t.co/knC8vwG9Se \n",
      "\n",
      "Breaking News : Diabetes Patients with Chest \u001b[31;2mPain\u001b[0m http : //t.co/kUMNl8PMIh \n",
      "\n",
      "RT @ 123_depression : Easing The Troubling \u001b[31;2mPain\u001b[0m Of Your Depression - http : //t.co/kcBwOfp4SQ \n",
      "\n",
      "New post : 3D Printed Mechanical \u001b[31;2mMuscle\u001b[0m Has a Heartbeat Powered by Yeast http : //t.co/l1Z70waLoz \n",
      "\n",
      "Study Shows \u001b[31;2mStatins\u001b[0m Harm More People Than They Help : http : //t.co/UhQiDW1vm3 # Obesity \n",
      "\n",
      "Study Shows \u001b[31;2mStatins\u001b[0m Harm More People Than They Help : http : //t.co/QQ0kAnvd # Obesity \n",
      "\n",
      "HEALTHY DIETS AND SCIENCE : \u001b[31;2mStatins\u001b[0m are associated with a 71 ... http : //t.co/XcwA4r3w \n",
      "\n"
     ]
    }
   ],
   "source": [
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
